{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67b12cfd",
   "metadata": {},
   "source": [
    "## Here, we will re-run the code modules written in the blog of [Word Vector Encoding](https://www.enjoyalgorithms.com/blog/word-vector-encoding-in-nlp/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f12d71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   UserName  ScreenName   Location     TweetAt  \\\n",
      "0      3799       48751     London  16-03-2020   \n",
      "1      3800       48752         UK  16-03-2020   \n",
      "2      3801       48753  Vagabonds  16-03-2020   \n",
      "3      3802       48754        NaN  16-03-2020   \n",
      "4      3803       48755        NaN  16-03-2020   \n",
      "\n",
      "                                       OriginalTweet           Sentiment  \n",
      "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
      "1  advice Talk to your neighbours family to excha...            Positive  \n",
      "2  Coronavirus Australia: Woolworths to give elde...            Positive  \n",
      "3  My food stock is not the only one which is emp...            Positive  \n",
      "4  Me, ready to go at supermarket during the #COV...  Extremely Negative  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweets = pd.read_csv('Corona_NLP_train.csv', encoding='ISO-8859-1')\n",
    "\n",
    "print(tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a8300bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets[['OriginalTweet', 'Sentiment']] #extraction\n",
    "tweets.columns = ['Text', 'Sentiment'] #renaming\n",
    "\n",
    "tweets['Text'] = tweets['Text'].str.lower()\n",
    "tweets['Text'] = tweets['Text'].str.replace(r\"http\\S+\", \"\", regex=True)\n",
    "\n",
    "tweets['Text'] = tweets['Text'].str.replace('[^A-Za-z0-9]+',' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46e272cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text           Sentiment\n",
      "0                       menyrbie phil gahan chrisitv             Neutral\n",
      "1  advice talk neighbours family exchange phone n...            Positive\n",
      "2  coronavirus australia woolworths give elderly ...            Positive\n",
      "3  food stock one empty please panic enough food ...            Positive\n",
      "4  ready go supermarket covid19 outbreak paranoid...  Extremely Negative\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "## NLTK library provides the set of stop words for English\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "tweets['Text'] = tweets['Text'].apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stopwords))\n",
    "print(tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dbf152b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>menyrbie phil gahan chrisitv</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[menyrbie, phil, gahan, chrisitv]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice talk neighbours family exchange phone n...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[advice, talk, neighbour, family, exchange, ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>coronavirus australia woolworths give elderly ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[coronavirus, australia, woolworth, give, elde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>food stock one empty please panic enough food ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[food, stock, one, empty, please, panic, enoug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ready go supermarket covid19 outbreak paranoid...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>[ready, go, supermarket, covid19, outbreak, pa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text           Sentiment  \\\n",
       "0                       menyrbie phil gahan chrisitv             Neutral   \n",
       "1  advice talk neighbours family exchange phone n...            Positive   \n",
       "2  coronavirus australia woolworths give elderly ...            Positive   \n",
       "3  food stock one empty please panic enough food ...            Positive   \n",
       "4  ready go supermarket covid19 outbreak paranoid...  Extremely Negative   \n",
       "\n",
       "                                   lemmatized_tokens  \n",
       "0                  [menyrbie, phil, gahan, chrisitv]  \n",
       "1  [advice, talk, neighbour, family, exchange, ph...  \n",
       "2  [coronavirus, australia, woolworth, give, elde...  \n",
       "3  [food, stock, one, empty, please, panic, enoug...  \n",
       "4  [ready, go, supermarket, covid19, outbreak, pa...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in  w_tokenizer.tokenize(text)]\n",
    "\n",
    "tweets['lemmatized_tokens'] = tweets['Text'].apply(lemmatize_text)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b3c2392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "tokens = []\n",
    "\n",
    "for i in range(len(tweets['lemmatized_tokens'])):\n",
    "    \n",
    "    for j in range(len(tweets['lemmatized_tokens'][i])):\n",
    "        \n",
    "        if tweets['lemmatized_tokens'][i][j] not in tokens:\n",
    "            \n",
    "            tokens.append(tweets['lemmatized_tokens'][i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f4c5918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51111\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a40cb0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "integer_label_encoded = label_encoder.fit_transform(tokens[1:10])\n",
    "\n",
    "label_encoded = integer_label_encoded.reshape(len(integer_label_encoded), 1)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "onehot_encoded = onehot_encoder.fit_transform(label_encoded)\n",
    "\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14e0aaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'Market' and 'Stock' - Continuous Bag of Word :  0.756814\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "CBOW = gensim.models.Word2Vec(tweets['lemmatized_tokens'], vector_size=10, window=5, min_count=1, workers=4)\n",
    "\n",
    "print(\"Cosine similarity between 'Market' \" + \"and 'Stock' - Continuous Bag of Word : \", CBOW.wv.similarity('market', 'stock'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5efb7257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'Market' and 'Stock' - Skip Gram:  0.87107706\n"
     ]
    }
   ],
   "source": [
    "CSG = gensim.models.Word2Vec(tweets['lemmatized_tokens'], vector_size=10, window=5, min_count=1, workers=4, sg=1)\n",
    "\n",
    "print(\"Cosine similarity between 'Market' \" + \"and 'Stock' - Skip Gram: \", CSG.wv.similarity('market', 'stock'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3bac5877",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ravishkumar/EA_venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.36782873, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = []\n",
    "    for item in tokens:\n",
    "        stems.append(PorterStemmer().stem(item))\n",
    "    return stems\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "tfs = tfidf.fit_transform(tweets[\"Text\"])\n",
    "\n",
    "# to visualize the formed TF-IDF matrix\n",
    "tfs.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "127f007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import collections\n",
    "import pandas as pd\n",
    "\n",
    "lemmatized_tokens = list(tweets[\"lemmatized_tokens\"])\n",
    "\n",
    "token_list = list(itertools.chain(*lemmatized_tokens))\n",
    "\n",
    "counts_no = collections.Counter(token_list)\n",
    "\n",
    "clean_tweets = pd.DataFrame(counts_no.most_common(30),\n",
    "                             columns=['words', 'count'])\n",
    "\n",
    "clean_tweets.sort_values(by='count')\n",
    "most_frequent_words = clean_tweets['words'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d58c967f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vectors = []\n",
    "\n",
    "for line in tweets['Text']:\n",
    "    tokens = nltk.word_tokenize(line)\n",
    "    vec = []\n",
    "    for token in most_frequent_words:\n",
    "        if token in tokens:\n",
    "            vec.append(1)\n",
    "        else:\n",
    "            vec.append(0)\n",
    "    vectors.append(vec)\n",
    "\n",
    "sentence_vectors = np.asarray(vectors)\n",
    "# Bag-of-Word Matrix\n",
    "sentence_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac4343c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
