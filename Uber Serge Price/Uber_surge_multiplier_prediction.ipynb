{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SO6qjCS-c3kZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#read the csv file using read_csv\n",
        "\n",
        "weather=pd.read_csv(\"N:\\Machine learning\\Algorithms\\weather.csv\",delimiter=',')\n",
        "rides=pd.read_csv(\"N:\\Machine learning\\Algorithms\\cab_rides.csv\",delimiter=',')\n",
        "\n",
        "\n",
        "#convert the timestamp into the desirable format\n",
        "\n",
        "rides['date_time'] = pd.to_datetime(rides['time_stamp']/1000, unit='s')\n",
        "weather['date_time'] = pd.to_datetime(weather['time_stamp'], unit='s')\n",
        "rides.head()\n",
        "\n",
        "#make a coloumn of merge date containing date merged with the location so that we can join the two dataframes on the basis of 'merge_date'\n",
        "\n",
        "rides['merge_date'] = rides.source.astype(str) +\" - \"+ rides.date_time.dt.date.astype(\"str\") +\" - \"+ rides.date_time.dt.hour.astype(\"str\")\n",
        "weather['merge_date'] = weather.location.astype(str) +\" - \"+ weather.date_time.dt.date.astype(\"str\") +\" - \"+ weather.date_time.dt.hour.astype(\"str\")\n",
        "\n",
        "\n",
        "# change the index to merge_date column so joining the two datasets will not generate any error.\n",
        "\n",
        "weather.index = weather['merge_date']\n",
        "\n",
        "final_dataframe = rides.join(weather,on=['merge_date'],rsuffix ='_w')\n",
        "\n",
        "#drop the null values rows\n",
        "\n",
        "final_dataframe=final_dataframe.dropna(axis=0)\n",
        "\n",
        "#make different columns of day and hour to simplify the format of date \n",
        "\n",
        "final_dataframe['day'] = final_dataframe.date_time.dt.dayofweek\n",
        "final_dataframe['hour'] = final_dataframe.date_time.dt.hour\n",
        "\n",
        "# we ignored surge value of more than 3 because the samples are very less for surge_multiplier>3\n",
        "\n",
        "surge_dataframe = final_dataframe[final_dataframe.surge_multiplier < 3]\n",
        "\n",
        "# feature selection--> we are selecting the most relevant features from the dataset\n",
        "\n",
        "x = surge_dataframe[['distance','day','hour','temp','clouds', 'pressure','humidity', 'wind', 'rain']]\n",
        "\n",
        "y=surge_dataframe['surge_multiplier']\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "#ignoring multiplier of 3 as there are only 2 values in our dataset\n",
        "le.fit([1,1.25,1.5,1.75,2.,2.25,2.5])\n",
        "y = le.transform(y) \n",
        "\n",
        "feature_list=list(x.columns)\n",
        "\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)\n",
        "\n",
        "# we are using smote to balance the data of different surge_multipliers\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "sm = SMOTE(random_state=42)\n",
        "x_train, y_train = sm.fit_resample(x_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "model= RandomForestClassifier(n_jobs=-1, random_state = 42,class_weight=\"balanced\")\n",
        "\n",
        "model.fit(x_train,y_train)\n",
        "y_pred=model.predict(x_test)\n",
        "\n",
        "# confusion_mat=confusion_matrix(y_test,y_pred,labels=None)\n",
        "# print(\"confusion_mat = \",confusion_mat)\n",
        "print(pd.crosstab(le.inverse_transform(y_test), le.inverse_transform(y_pred),rownames=['Actual'],colnames=['Predicted']))\n",
        "\n",
        "print(\"Accuracy Score = \",accuracy_score(y_test,y_pred))              \n",
        "print(\"precision score = \",precision_score(y_test, y_pred,average='weighted'))         \n",
        "print(\"recall score = \",recall_score(y_test, y_pred,average='micro'))               \n",
        "from sklearn.metrics import f1_score\n",
        "print(\"f1 score = \",f1_score(y_test, y_pred, average='weighted'))\n",
        "errors = abs(y_pred - y_test)\n",
        "\n",
        "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n",
        "\n",
        "\n",
        "# Get numerical feature importances\n",
        "importances = list(model.feature_importances_)\n",
        "# List of tuples with variable and importance\n",
        "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
        "# Sort the feature importances by most important first\n",
        "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
        "# Print out the feature and importances \n",
        "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]"
      ]
    }
  ]
}